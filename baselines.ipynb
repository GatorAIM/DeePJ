{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we compare different baseline models with DeePJ. These baseline models include:  \n",
    "1. Bi-directional LSTM and GRU\n",
    "2. Deepr\n",
    "3. GCN with P\n",
    "4. GAT with P\n",
    "5. Transformer  \n",
    "6. Graph Conv Transformer (GCT)\n",
    "7. HiTANet, if time permitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch_geometric.nn import DenseGraphConv, DenseGCNConv\n",
    "from pyhealth.models import (RNNLayer, DeeprLayer)\n",
    "from copy import deepcopy as c\n",
    "from process_eicu import get_eicu_dataset\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from scipy.stats import sem, t\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, \n",
    "                             average_precision_score)\n",
    "import importlib\n",
    "import DeePJ\n",
    "import DeePJ_ablation\n",
    "import Transformer\n",
    "import GCT\n",
    "importlib.reload(DeePJ)\n",
    "importlib.reload(DeePJ_ablation)\n",
    "importlib.reload(Transformer)\n",
    "importlib.reload(GCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset(all_pat_id, all_code_ints, all_pad_masks, all_causal_masks, all_intervals, prior_matrices, all_labels)\n",
    "dataset, enc_dict, pat_dict, all_str2int, pat_id_mapping = get_eicu_dataset('./eicu_full/', 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some stastistics\n",
    "labels = []\n",
    "max_elapsed_time = 0\n",
    "for pat in dataset:\n",
    "    labels.append(pat[-1].item())\n",
    "    elapsed_time = torch.sum(pat[4]).item()\n",
    "    if elapsed_time > max_elapsed_time:\n",
    "        max_elapsed_time = elapsed_time\n",
    "        \n",
    "print(f'max_elapsed_time: {max_elapsed_time} minutes')\n",
    "num_classes = np.unique(np.array(labels)).size\n",
    "print(f'num_classes: {num_classes}')\n",
    "# get the pad token\n",
    "pad_token = len(all_str2int)\n",
    "print(f'pad_token: {pad_token}')\n",
    "# add 1 to vocab size for the pad token\n",
    "vocab_size = len(all_str2int) + 1\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "seq_len = len(dataset[0][1])\n",
    "print(f'seq_len: {seq_len}')\n",
    "max_num_encs = list(pat_dict.values())[0].max_num_encs\n",
    "print(f'max_num_encs: {max_num_encs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_classifier_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): The number of input features.\n",
    "            num_classes (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, input_dim) for _ in range(num_classifier_layers - 1)] \n",
    "            + [nn.Linear(input_dim, num_classes)]\n",
    "        )\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)  # LogSoftmax activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_dim]\n",
    "        Returns:\n",
    "            log_probs: Log probabilities of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        if len(self.linears) > 1:\n",
    "            for linear in self.linears[:-1]:\n",
    "                x = F.relu(linear(x))  # ReLU activation\n",
    "        logits = self.linears[-1](x)  # Call the last layer explicitly\n",
    "        log_probs = self.log_softmax(logits)  # Apply LogSoftmax\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, gnn_type, d_model, num_gnn_layers, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gnn_type: str, either 'GCN' or 'GraphConv', specifying which type of GNN to use.\n",
    "            d_model: int, dimension of node features.\n",
    "            num_gnn_layers: int, number of GNN layers.\n",
    "            dropout: float, dropout probability.\n",
    "        \"\"\"\n",
    "        super(GNN, self).__init__()\n",
    "        self.gnn_type = gnn_type\n",
    "        self.d_model = d_model\n",
    "        self.num_gnn_layers = num_gnn_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Select the appropriate convolutional layer type based on gnn_type\n",
    "        if gnn_type == \"GCN\":\n",
    "            self.conv_layers = nn.ModuleList([DenseGCNConv(d_model, d_model) for _ in range(num_gnn_layers)])\n",
    "        elif gnn_type == \"GraphConv\":\n",
    "            self.conv_layers = nn.ModuleList([DenseGraphConv(d_model, d_model) for _ in range(num_gnn_layers)])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported GNN type: {gnn_type}\")\n",
    "\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(d_model) for _ in range(num_gnn_layers)])\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, num_nodes, d_model), node features.\n",
    "            adj: Tensor of shape (batch_size, num_nodes, num_nodes), adjacency matrix.\n",
    "            mask: Tensor of shape (batch_size, num_nodes, num_nodes), adjacency mask.\n",
    "            add_loop: bool, whether to add self-loops to the adjacency matrix.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, d_model), graph-level representation.\n",
    "        \"\"\"\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            # Apply the convolution layer\n",
    "            out = conv(x=x, adj=adj, mask=mask)\n",
    "\n",
    "            # Residual connection\n",
    "            out = out + x\n",
    "\n",
    "            # Batch normalization\n",
    "            out = self.batch_norms[i](out.transpose(1, 2)).transpose(1, 2)  # (B, N, d_model) -> (B, d_model, N) -> (B, N, d_model)\n",
    "\n",
    "            # ReLU activation\n",
    "            out = self.relu(out)\n",
    "\n",
    "            # Dropout\n",
    "            out = self.dropout_layer(out)\n",
    "\n",
    "            # Update x for the next layer\n",
    "            x = out  # (batch_size, num_nodes, d_model)\n",
    "\n",
    "        # Perform global mean pooling to obtain graph-level features\n",
    "        readout = torch.mean(out, dim=1)  # Average pooling across nodes\n",
    "        return readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, embedder, encoder, classifier):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedder(x)\n",
    "    \n",
    "    def encode(self, x, pad_masks, causal_masks = None, priors = None):\n",
    "        # the input is determined by the type of encoder\n",
    "        # if time series model, take the average of the embedding of the encounter \n",
    "        if isinstance(self.encoder, RNNLayer):\n",
    "            x = x.view(x.size(0), max_num_encs, -1, x.size(2))  \n",
    "            x = torch.mean(x, dim=2)  # shape [batch_size, max_num_encs * 2, d_model]\n",
    "            output = self.encoder(x, mask = None)\n",
    "        elif isinstance(self.encoder, GNN):\n",
    "            output = self.encoder(x = x, adj = priors, mask = pad_masks)\n",
    "        elif isinstance(self.encoder, Transformer.Transformer):\n",
    "            output = self.encoder(seq_emds = x, pad_masks = pad_masks, causal_masks = causal_masks)\n",
    "        else:\n",
    "            output = self.encoder(x, pad_masks)\n",
    "\n",
    "        # the output is determined by the type of encoder\n",
    "        if isinstance(self.encoder, RNNLayer):\n",
    "            output = output[1]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def classify(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def forward(self, x, pad_masks, causal_masks = None, priors = None):\n",
    "        x = self.embed(x = x) \n",
    "        x = self.encode(x = x, pad_masks = pad_masks, causal_masks = causal_masks, priors = priors) \n",
    "        logits = self.classify(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineGCT(nn.Module):\n",
    "    def __init__(self, embedder, encoder, classifier):\n",
    "        super(BaselineGCT, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedder(x)\n",
    "    \n",
    "    def encode(self, x, pad_masks, causal_masks, priors):\n",
    "        output = self.encoder(x, pad_masks, causal_masks, priors)\n",
    "        return output\n",
    "    def classify(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def forward(self, x, pad_masks, causal_masks, priors):\n",
    "        x = self.embed(x = x) \n",
    "        x, KLD_loss = self.encode(x = x, pad_masks = pad_masks, causal_masks = causal_masks, priors = priors) \n",
    "        logits = self.classify(x)\n",
    "        return logits, KLD_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_models(hp_dict, device):\n",
    "    embedder = Embedder(d_model = hp_dict['d_model'], vocab_size = hp_dict['vocab_size'])\n",
    "    classifier = LinearClassifier(input_dim = hp_dict['d_model'], num_classes = hp_dict['num_classes'], \n",
    "                                  num_classifier_layers = hp_dict['num_classifier_layers'])\n",
    "    gru_layers = RNNLayer(input_size = hp_dict['d_model'], hidden_size = hp_dict['d_model'], rnn_type = 'GRU', \n",
    "                          num_layers = hp_dict['num_encoder_layers'], dropout = hp_dict['dropout'], bidirectional = False)\n",
    "    lstm_layers = RNNLayer(input_size = hp_dict['d_model'], hidden_size = hp_dict['d_model'], rnn_type = 'LSTM', \n",
    "                          num_layers = hp_dict['num_encoder_layers'], dropout = hp_dict['dropout'], bidirectional = False)\n",
    "    tf_layers = Transformer.Transformer(d_model = hp_dict['d_model'], dropout = hp_dict['dropout'], num_layers = hp_dict['num_encoder_layers'])\n",
    "    deepr_layers = DeeprLayer(feature_size = hp_dict['d_model'], hidden_size = hp_dict['d_model'])\n",
    "    gcn_layers = GNN(gnn_type = 'GCN', d_model = hp_dict['d_model'], num_gnn_layers = hp_dict['num_gnn_layers'], \n",
    "                     dropout = hp_dict['dropout'])\n",
    "    graphconv_layers = GNN(gnn_type = 'GraphConv', d_model = hp_dict['d_model'], num_gnn_layers = hp_dict['num_gnn_layers'], \n",
    "                           dropout = hp_dict['dropout'])\n",
    "    gct_layers = GCT.GCT(d_model = hp_dict['d_model'], dropout = hp_dict['dropout'], num_layers = hp_dict['num_encoder_layers'])\n",
    "\n",
    "\n",
    "    deepr = BaselineModel(c(embedder), c(deepr_layers), c(classifier)).to(device)\n",
    "    gru = BaselineModel(c(embedder), c(gru_layers), c(classifier)).to(device)\n",
    "    lstm = BaselineModel(c(embedder), c(lstm_layers), c(classifier)).to(device)\n",
    "    tf = BaselineModel(c(embedder), c(tf_layers), c(classifier)).to(device)\n",
    "    gcn = BaselineModel(c(embedder), c(gcn_layers), c(classifier)).to(device)\n",
    "    graph_conv = BaselineModel(c(embedder), c(graphconv_layers), c(classifier)).to(device)\n",
    "\n",
    "    gct = BaselineGCT(c(embedder), c(gct_layers), c(classifier)).to(device)\n",
    "    \n",
    "\n",
    "    \n",
    "    deepj_wo_SL = DeePJ_ablation.make_deepj_ablation_model(hp_dict['d_model'], hp_dict['num_encoder_layers'], hp_dict['num_classifier_layers'], \n",
    "                                   hp_dict['dropout'], hp_dict['vocab_size'], hp_dict['max_elapsed_time'], hp_dict['num_deepj_graph_clusters'],\n",
    "                                   hp_dict['num_classes'], use_TE = True, use_SL = False, use_GP = True).to(device)\n",
    "    deepj_wo_GP = DeePJ_ablation.make_deepj_ablation_model(hp_dict['d_model'], hp_dict['num_encoder_layers'], hp_dict['num_classifier_layers'], \n",
    "                                   hp_dict['dropout'], hp_dict['vocab_size'], hp_dict['max_elapsed_time'], hp_dict['num_deepj_graph_clusters'],\n",
    "                                   hp_dict['num_classes'], use_TE = True, use_SL = True, use_GP = False).to(device)\n",
    "    deepj = DeePJ.make_deepj_model(hp_dict['d_model'], hp_dict['num_encoder_layers'], hp_dict['num_classifier_layers'], \n",
    "                                   hp_dict['dropout'], hp_dict['vocab_size'], hp_dict['max_elapsed_time'], hp_dict['num_deepj_graph_clusters'],\n",
    "                                   hp_dict['num_classes']).to(device)\n",
    "    \n",
    "    return {'GraphConv': graph_conv, 'GCN': gcn, 'GRU': gru, 'LSTM': lstm, 'Deepr': deepr, 'Transformer': tf, 'GCT': gct, \n",
    "            'DeePJ/SL': deepj_wo_SL, 'DeePJ/GP': deepj_wo_GP, 'DeePJ': deepj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_dict, train_loader, val_loader, \n",
    "          optimizer_dict, scheduler_dict, hp_dict, device):\n",
    "    best_val_AUPRC_dict = {name: 0 for name in model_dict.keys()}\n",
    "    best_model_dict = {name: None for name in model_dict.keys()}\n",
    "    best_model_epoch_dict = {name: 0 for name in model_dict.keys()}   \n",
    "    \n",
    "    # train the model\n",
    "    num_epochs = hp_dict['epochs']\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"=====================================================================\")\n",
    "        print(f\"\\tEpoch {epoch + 1}/{num_epochs}\")\n",
    "        for name, model in model_dict.items():\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                # get the batch data and move it to the device\n",
    "                batch = [item.to(device) for item in batch]\n",
    "                _, all_code_ints_batch, all_pad_masks_batch, all_causal_masks_batch, \\\n",
    "                    all_intervals_batch, prior_matrices_batch, all_labels_batch = batch\n",
    "                \n",
    "                # zero the parameter gradients for each model's optimizer\n",
    "                optimizer_dict[name].zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                if isinstance(model, BaselineModel):\n",
    "                    logits = model(x = all_code_ints_batch, pad_masks = all_pad_masks_batch, \n",
    "                                   causal_masks = all_causal_masks_batch, priors = prior_matrices_batch)\n",
    "                    # compute the loss\n",
    "                    loss = F.nll_loss(logits, all_labels_batch)\n",
    "                \n",
    "                elif isinstance(model, BaselineGCT):\n",
    "                    logits, KLD_loss = model(x = all_code_ints_batch, pad_masks = all_pad_masks_batch, \n",
    "                                             causal_masks = all_causal_masks_batch, priors = prior_matrices_batch)\n",
    "                    # compute the loss\n",
    "                    cls_loss = F.nll_loss(logits, all_labels_batch)\n",
    "                    loss = cls_loss + hp_dict['GCT_KLD_coef'] * KLD_loss\n",
    "                    \n",
    "                # if the model is an ablation model, we need to handle the loss differently\n",
    "                elif isinstance(model, DeePJ.DeepJourney) or (isinstance(model, DeePJ_ablation.DeepJourneyAblation) and model.use_TE == False):\n",
    "                    logits, _, _, _, KLD_loss, link_loss, ent_loss = model(code_ints = all_code_ints_batch, \n",
    "                                                                           pad_masks = all_pad_masks_batch, \n",
    "                                                                           causal_masks = all_causal_masks_batch, \n",
    "                                                                           enc_intervals = all_intervals_batch, \n",
    "                                                                           prior_matrices = prior_matrices_batch)\n",
    "                    # compute the loss\n",
    "                    cls_loss = F.nll_loss(logits, all_labels_batch)\n",
    "                    weighted_KLD_loss = hp_dict['deepj_KLD_coef'] * KLD_loss\n",
    "                    weighted_link_loss = hp_dict['deepj_link_coef'] * link_loss\n",
    "                    weighted_ent_loss = hp_dict['deepj_ent_coef'] * ent_loss\n",
    "                    loss = cls_loss + weighted_KLD_loss + weighted_link_loss + weighted_ent_loss\n",
    "                \n",
    "                \n",
    "                elif isinstance(model, DeePJ_ablation.DeepJourneyAblation) and model.use_SL == False:\n",
    "                    logits, _, _, _, link_loss, ent_loss = model(code_ints = all_code_ints_batch,\n",
    "                                                                 pad_masks = all_pad_masks_batch,\n",
    "                                                                 causal_masks = all_causal_masks_batch, \n",
    "                                                                 enc_intervals = all_intervals_batch,\n",
    "                                                                 prior_matrices = prior_matrices_batch)\n",
    "                    # compute the loss\n",
    "                    cls_loss = F.nll_loss(logits, all_labels_batch)\n",
    "                    weighted_link_loss = hp_dict['deepj_link_coef'] * link_loss\n",
    "                    weighted_ent_loss = hp_dict['deepj_ent_coef'] * ent_loss\n",
    "                    loss = cls_loss + weighted_link_loss + weighted_ent_loss\n",
    "                \n",
    "                elif isinstance(model, DeePJ_ablation.DeepJourneyAblation) and model.use_GP == False:\n",
    "                    logits, _, KLD_loss = model(code_ints = all_code_ints_batch,\n",
    "                                                pad_masks = all_pad_masks_batch,\n",
    "                                                causal_masks = all_causal_masks_batch, \n",
    "                                                enc_intervals = all_intervals_batch,\n",
    "                                                prior_matrices = prior_matrices_batch)\n",
    "                    # compute the loss\n",
    "                    cls_loss = F.nll_loss(logits, all_labels_batch)\n",
    "                    weighted_KLD_loss = hp_dict['deepj_KLD_coef'] * KLD_loss\n",
    "                    loss = cls_loss + weighted_KLD_loss\n",
    "            \n",
    "                # all the loss conditions are covered, run the backward pass\n",
    "                loss.backward()\n",
    "                optimizer_dict[name].step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            scheduler_dict[name].step()\n",
    "            current_lr = optimizer_dict[name].param_groups[0]['lr']\n",
    "            print(f\"\\t{name}, Loss: {train_loss / len(train_loader):.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "            # test the model on the validation set, if the validation AUPRC is improved, save the model\n",
    "            _ = test(model, train_loader, 'train')\n",
    "            val_result = test(model, val_loader, 'val')\n",
    "            if val_result['AUPRC'] > best_val_AUPRC_dict[name]:\n",
    "                best_val_AUPRC_dict[name] = val_result['AUPRC']\n",
    "                best_model_dict[name] = c(model)\n",
    "                best_model_epoch_dict[name] = epoch + 1\n",
    "                  \n",
    "    print(f'\\tBest model saved at epoch:', best_model_epoch_dict)\n",
    "    return best_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, mode = 'val'):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = [item.to(device) for item in batch]\n",
    "            _, all_code_ints_batch, all_pad_masks_batch, all_causal_masks_batch, \\\n",
    "                all_intervals_batch, prior_matrices_batch, all_labels_batch = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            if isinstance(model, BaselineModel):\n",
    "                logits = model(x = all_code_ints_batch, pad_masks = all_pad_masks_batch, \n",
    "                               causal_masks = all_causal_masks_batch, priors = prior_matrices_batch)\n",
    "            elif isinstance(model, BaselineGCT):\n",
    "                logits, _ = model(x = all_code_ints_batch, pad_masks = all_pad_masks_batch, \n",
    "                                  causal_masks = all_causal_masks_batch, priors = prior_matrices_batch)\n",
    "            elif isinstance(model, DeePJ.DeepJourney) or isinstance(model, DeePJ_ablation.DeepJourneyAblation):\n",
    "                outputs = model(code_ints = all_code_ints_batch, \n",
    "                                pad_masks = all_pad_masks_batch, \n",
    "                                causal_masks = all_causal_masks_batch, \n",
    "                                enc_intervals = all_intervals_batch, \n",
    "                                prior_matrices = prior_matrices_batch)\n",
    "                logits = outputs[0]\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Store results\n",
    "            all_labels.extend(all_labels_batch.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_logits.extend(logits.cpu().numpy())  # Logits are log softmaxed\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_logits = np.array(all_logits)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = np.mean(all_labels == all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Compute AUROC\n",
    "    if len(np.unique(all_labels)) > 2:\n",
    "        # Multi-class case\n",
    "        probs = np.exp(all_logits)  # Convert log softmaxed logits to probabilities\n",
    "        AUROC = roc_auc_score(all_labels, probs, multi_class='ovr')\n",
    "        AUPRC = average_precision_score(all_labels, probs, average='macro')\n",
    "    else:\n",
    "        # Binary case\n",
    "        probs = np.exp(all_logits)[:, 1]  # Probability for the positive class\n",
    "        AUROC = roc_auc_score(all_labels, probs)\n",
    "        AUPRC = average_precision_score(all_labels, probs)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\t\\tOn {mode} set - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, AUROC: {AUROC:.4f}, AUPRC: {AUPRC:.4f}\")\n",
    "    \n",
    "    return {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1, 'AUROC': AUROC, 'AUPRC': AUPRC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_results_statistics(fold_test_results: dict, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Organize a list of metric dictionaries into a dict with mean and confidence interval ranges.\n",
    "    \n",
    "    Args:\n",
    "        results_list (list of dict): List of dictionaries with performance metrics.\n",
    "        confidence_level (float): Confidence level for the confidence interval (default 0.95).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Organized dictionary with metrics, mean, and confidence intervals as ranges.\n",
    "    \"\"\"\n",
    "    # for each model, we derive the statistics of the performance metrics across the folds\n",
    "    model_performance = {}\n",
    "    for (name, results_list) in fold_test_results.items():\n",
    "        # Initialize the results dictionary\n",
    "        aggregated_results = {}\n",
    "        \n",
    "        # Extract all metrics\n",
    "        metrics = results_list[0].keys()\n",
    "        \n",
    "        # Calculate mean and CI for each metric\n",
    "        for metric in metrics:\n",
    "            # Extract values for the current metric\n",
    "            values = np.array([res[metric] for res in results_list])\n",
    "            \n",
    "            # Calculate mean\n",
    "            mean = np.mean(values)\n",
    "            \n",
    "            # Calculate confidence interval range\n",
    "            n = len(values)\n",
    "            stderr = sem(values)\n",
    "            t_critical = t.ppf((1 + confidence_level) / 2, df=n-1)  # Two-tailed t critical value\n",
    "            margin = t_critical * stderr\n",
    "            lower_bound = mean - margin\n",
    "            upper_bound = mean + margin\n",
    "            \n",
    "            # Store in the dictionary\n",
    "            aggregated_results[metric] = {\n",
    "                'mean': f\"{mean:.4f}\",\n",
    "                '95% CI': f\"{lower_bound:.4f} - {upper_bound:.4f}\"\n",
    "            }\n",
    "        model_performance[name] = aggregated_results\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(dataset: TensorDataset, n_splits: int, hp_dict: dict, device):\n",
    "    # Set up k-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=888)\n",
    "    ys = np.array([pat[-1].item() for pat in dataset]) # get the labels for stratifiedKFold\n",
    "    fold_test_results = {}\n",
    "    \n",
    "    for fold, (train_val_idx, test_idx) in enumerate(kf.split(dataset, ys)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split train+val indices into train and validation sets\n",
    "        train_size = int(0.8 * len(train_val_idx))\n",
    "        val_size = len(train_val_idx) - train_size\n",
    "        train_idx, val_idx = torch.utils.data.random_split(train_val_idx, \n",
    "                                                           [train_size, val_size], \n",
    "                                                           torch.Generator().manual_seed(42))\n",
    "        \n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=hp_dict['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "        test_loader = DataLoader(Subset(dataset, test_idx), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "        \n",
    "        model_dict = make_all_models(hp_dict, device)\n",
    "        \n",
    "        optimizer_dict = {}\n",
    "        scheduler_dict = {}\n",
    "        for name, model in model_dict.items():\n",
    "            optimizer_dict[name] = torch.optim.Adam(model.parameters(), lr=hp_dict['lr'], weight_decay=hp_dict['weight_decay'])\n",
    "            scheduler_dict[name] = optim.lr_scheduler.StepLR(optimizer_dict[name], step_size=hp_dict['scheduler_step'], gamma=hp_dict['scheduler_rate'])\n",
    "            \n",
    "        # best model during the training in this fold\n",
    "        best_model_fold_dict = train(model_dict, train_loader, val_loader, optimizer_dict, \n",
    "                                     scheduler_dict, hp_dict, device = device)\n",
    "        \n",
    "        for name, best_model_fold in best_model_fold_dict.items():\n",
    "            test_result_dict = test(best_model_fold, test_loader, 'test')\n",
    "            if name not in fold_test_results.keys():\n",
    "                fold_test_results[name] = []\n",
    "            fold_test_results[name].append(test_result_dict)\n",
    "            \n",
    "    return CV_results_statistics(fold_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    # common model hyper-parameters\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 256,\n",
    "    'num_encoder_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'num_classifier_layers': 1,\n",
    "    'num_classes': num_classes,\n",
    "    # gct hyper-parameters\n",
    "    'GCT_KLD_coef': 1.0,\n",
    "    # deepj hyper-parameters\n",
    "    'max_elapsed_time': max_elapsed_time,\n",
    "    'num_deepj_graph_clusters': 3,\n",
    "    'deepj_KLD_coef': 1.0,\n",
    "    'deepj_link_coef': 1.0,\n",
    "    'deepj_ent_coef': 1.0,\n",
    "    # gnn hyper-parameters\n",
    "    'num_gnn_layers': 5,\n",
    "    # training hyper-parameters\n",
    "    'batch_size': 256,\n",
    "    'epochs': 30,\n",
    "    'lr': 5e-4,\n",
    "    'scheduler_step': 8,\n",
    "    'scheduler_rate': 0.8,\n",
    "    'weight_decay': 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cv = cross_validation(dataset, 10, hp_dict, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerize_cv_results(result_cv):\n",
    "    # Convert the results to a DataFrame\n",
    "    rows = {}\n",
    "    for model, metrics in result_cv.items():\n",
    "        row = {}\n",
    "        for metric, values in metrics.items():\n",
    "            row[metric] = f\"{values['mean']} ({values['95% CI']})\"\n",
    "        rows[model] = row\n",
    "    final_result_df = pd.DataFrame.from_dict(rows, orient='index')\n",
    "    return final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df = summerize_cv_results(result_cv)\n",
    "final_result_df.to_csv('final_result_eICU.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
