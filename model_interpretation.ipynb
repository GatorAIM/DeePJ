{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import textwrap\n",
    "from matplotlib import pyplot as plt\n",
    "from process_eicu import get_eicu_dataset\n",
    "import importlib\n",
    "import DeePJ\n",
    "import train\n",
    "import utils\n",
    "importlib.reload(DeePJ)\n",
    "importlib.reload(train)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset(all_pat_id, all_code_ints, all_pad_masks, all_causal_masks, all_intervals, prior_matrices, all_labels)\n",
    "dataset, enc_dict, pat_dict, all_str2int, pat_id_mapping = get_eicu_dataset('./eicu_full/', 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some stastistics\n",
    "labels = []\n",
    "max_elapsed_time = 0\n",
    "for pat in dataset:\n",
    "    labels.append(pat[-1].item())\n",
    "    elapsed_time = pat[4][-1].item()\n",
    "    if elapsed_time > max_elapsed_time:\n",
    "        max_elapsed_time = elapsed_time\n",
    "\n",
    "num_samples = len(dataset)\n",
    "print(f'num_samples: {num_samples}')        \n",
    "print(f'max_elapsed_time: {max_elapsed_time} minutes')\n",
    "num_classes = np.unique(np.array(labels)).size\n",
    "print(f'num_classes: {num_classes}')\n",
    "# get the pad token\n",
    "pad_token = len(all_str2int)\n",
    "print(f'pad_token: {pad_token}')\n",
    "# add 1 to vocab size for the pad token\n",
    "vocab_size = len(all_str2int) + 1\n",
    "print(f'vocab_size: {vocab_size}')\n",
    "seq_len = len(dataset[0][1])\n",
    "print(f'seq_len: {seq_len}')\n",
    "max_num_encs = list(pat_dict.values())[0].max_num_encs\n",
    "print(f'max_num_encs: {max_num_encs}')\n",
    "max_num_codes = 50\n",
    "print(f'max_num_codes: {max_num_codes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    # common model hyper-parameters\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 256,\n",
    "    'num_encoder_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'num_classifier_layers': 1,\n",
    "    'num_classes': num_classes,\n",
    "    # gct hyper-parameters\n",
    "    'GCT_KLD_coef': 1.0,\n",
    "    # deepj hyper-parameters\n",
    "    'max_elapsed_time': max_elapsed_time,\n",
    "    'num_deepj_graph_clusters': 3,\n",
    "    'deepj_KLD_coef': 1.0,\n",
    "    'deepj_link_coef': 1.0,\n",
    "    'deepj_ent_coef': 1.0,\n",
    "    # gnn hyper-parameters\n",
    "    'num_gnn_layers': 5,\n",
    "    # training hyper-parameters\n",
    "    'batch_size': 256,\n",
    "    'epochs': 20,\n",
    "    'lr': 5e-4,\n",
    "    'scheduler_step': 8,\n",
    "    'scheduler_rate': 0.8,\n",
    "    'weight_decay': 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepj = DeePJ.make_deepj_model(hp_dict['d_model'], hp_dict['num_encoder_layers'], hp_dict['num_classifier_layers'], \n",
    "                                   hp_dict['dropout'], hp_dict['vocab_size'], hp_dict['max_elapsed_time'], hp_dict['num_deepj_graph_clusters'],\n",
    "                                   hp_dict['num_classes']).to(device)\n",
    "model_dict = {'deepj': deepj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "indices = np.arange(dataset_size)\n",
    "# Split into train (70%) and temp (30%)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.7, random_state=42, shuffle=True)\n",
    "# Split temp into validation (10%) and test (20%)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=2/3, random_state=42, shuffle=True)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(Subset(dataset, train_idx), batch_size=hp_dict['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(Subset(dataset, val_idx), batch_size=hp_dict['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(Subset(dataset, test_idx), batch_size=hp_dict['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict = {'deepj': torch.optim.Adam(model_dict['deepj'].parameters(), lr=hp_dict['lr'], weight_decay=hp_dict['weight_decay'])}\n",
    "scheduler_dict = {'deepj': optim.lr_scheduler.StepLR(optimizer_dict['deepj'], step_size=hp_dict['scheduler_step'], gamma=hp_dict['scheduler_rate'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dict = train.train(model_dict, train_loader, val_loader, optimizer_dict, scheduler_dict, hp_dict, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_int2str = {\n",
    "    value: '|'.join(key.split('|')[-2:])  # Join the last two parts with '|'\n",
    "    for key, value in all_str2int.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten the all_int2str\n",
    "all_int2str_short = {}\n",
    "for k, v in all_int2str.items():\n",
    "    if len(v) < 50:\n",
    "        short_v = v\n",
    "    elif v == 'was the patient admitted from the o.r. or went to the o.r. within 4 hours of admission?|no':\n",
    "        short_v = \"No ICU admission within 4 hours\"  \n",
    "    elif v == 'was the patient admitted from the o.r. or went to the o.r. within 4 hours of admission?|yes':\n",
    "        short_v = \"Recent ICU admission within 4 hours\"  \n",
    "    else:\n",
    "        last = v.split('|')[-1]\n",
    "        if last == 'no' or last == 'yes':\n",
    "            short_v = v\n",
    "        else:\n",
    "            short_v = last\n",
    "    all_int2str_short[k] = short_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model_dict['deepj'].to('cpu')\n",
    "count = 0\n",
    "for sample in dataset:\n",
    "    pat_id, code_ints, pad_masks, causal_masks, intervals, prior_matrices, labels = sample\n",
    "    # we only plot the graphs of the patients whose have more than 2 encounters\n",
    "    if intervals[-1] != 0 and labels.item() == 1:\n",
    "        with torch.no_grad():\n",
    "            _, adj_matrices, cluster_assign, cluster_weights, _, _, _ = best_model(code_ints.unsqueeze(0), \n",
    "                                                                                pad_masks.unsqueeze(0), \n",
    "                                                                                causal_masks.unsqueeze(0), \n",
    "                                                                                intervals.unsqueeze(0), \n",
    "                                                                                prior_matrices.unsqueeze(0))\n",
    "        adj_matrix = adj_matrices[0].detach().numpy()\n",
    "        cluster_hard = cluster_assign.argmax(dim=-1)[0].detach().numpy()\n",
    "        \n",
    "        utils.plot_graph(adj_matrix, intervals, cluster_hard, cluster_weights, all_int2str_short, 0.1, code_ints)\n",
    "        count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expired_patients = []\n",
    "# we only need expried patients\n",
    "for sample in dataset:\n",
    "    pat_id, code_ints, pad_masks, causal_masks, intervals, prior_matrices, labels = sample\n",
    "    if labels.item() == 1:\n",
    "        expired_patients.append(sample)\n",
    "print(len(expired_patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_above_threshold(adj_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Find the coordinates and values of all entries in an array (NumPy or PyTorch)\n",
    "    that are greater than the given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - adj_matrix (numpy.ndarray or torch.Tensor): Input adjacency matrix (NumPy or PyTorch tensor).\n",
    "    - threshold (float): Threshold value.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: [((row, col), value)] where value > threshold.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch tensor to NumPy if necessary\n",
    "    if isinstance(adj_matrix, torch.Tensor):\n",
    "        adj_matrix = adj_matrix.cpu().numpy()\n",
    "\n",
    "    # Find coordinates where values exceed the threshold\n",
    "    coordinates = np.argwhere(adj_matrix > threshold)\n",
    "\n",
    "    # Store ((row, col), value) pairs\n",
    "    coordinates = [((int(row), int(col)), float(adj_matrix[row, col])) for row, col in coordinates]\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_enc_check(coordinates, max_num_codes):\n",
    "    # filter out those inter-encounter connections\n",
    "    cleaned_coordinates = []\n",
    "    for corr in coordinates:\n",
    "        if (corr[0][0] // (max_num_codes * 2)) == (corr[0][1] // (max_num_codes * 2)):\n",
    "            cleaned_coordinates.append(corr)\n",
    "    return cleaned_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_enc_check(coordinates, max_num_codes):\n",
    "    # filter out those intra-encounter connections\n",
    "    cleaned_coordinates = []\n",
    "    for corr in coordinates:\n",
    "        if (corr[0][0] // (max_num_codes * 2)) != (corr[0][1] // (max_num_codes * 2)):\n",
    "            cleaned_coordinates.append(corr)\n",
    "    return cleaned_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_coors_to_ints(coordinates, code_ints, pad_token):\n",
    "    if isinstance(code_ints, torch.Tensor):\n",
    "        code_ints = code_ints.cpu().numpy()\n",
    "    translated_coordinates = []\n",
    "    for coor in coordinates:\n",
    "        translated_coordinates.append(((code_ints[coor[0][0]], code_ints[coor[0][1]]), coor[1]))\n",
    "    # sanity check\n",
    "    for coor in translated_coordinates:\n",
    "        assert coor[0][0] != pad_token and coor[0][1] != pad_token\n",
    "    return translated_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_edge_dict_update(coordinates, enc_edge_dict):\n",
    "    for coor in coordinates:\n",
    "        if coor[0] not in enc_edge_dict.keys():\n",
    "            enc_edge_dict[coor[0]] = []\n",
    "        enc_edge_dict[coor[0]].append(coor[1])\n",
    "    return enc_edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_enc_edge_dict = {}\n",
    "inter_enc_edge_dict = {}\n",
    "for sample in tqdm(expired_patients):\n",
    "    _, code_ints, pad_masks, causal_masks, intervals, prior_matrices, labels = sample\n",
    "    with torch.no_grad():\n",
    "        _, adj_matrices, cluster_assign, cluster_weights, _, _, _ = best_model(code_ints.unsqueeze(0), \n",
    "                                                                            pad_masks.unsqueeze(0), \n",
    "                                                                            causal_masks.unsqueeze(0), \n",
    "                                                                            intervals.unsqueeze(0), \n",
    "                                                                            prior_matrices.unsqueeze(0))\n",
    "    coordinates = get_edges_above_threshold(adj_matrices[0], 0.1)\n",
    "    intra_cleaned_coordinates = intra_enc_check(coordinates, max_num_codes)\n",
    "    inter_cleaned_coordinates = inter_enc_check(coordinates, max_num_codes)\n",
    "    intra_translated_coordinates = translate_coors_to_ints(intra_cleaned_coordinates, code_ints, pad_token)\n",
    "    inter_translated_coordinates = translate_coors_to_ints(inter_cleaned_coordinates, code_ints, pad_token)\n",
    "    intra_enc_edge_dict = enc_edge_dict_update(intra_translated_coordinates, intra_enc_edge_dict)\n",
    "    inter_enc_edge_dict = enc_edge_dict_update(inter_translated_coordinates, inter_enc_edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique encounters and encounter gaps\n",
    "# that is, each patient can have 1 to max_num_encs unique encounters and max_num_encs - 1 encounter gaps\n",
    "num_unq_encs = 0\n",
    "num_enc_gaps = 0\n",
    "for sample in tqdm(expired_patients):\n",
    "    pat_id, code_ints, pad_masks, causal_masks, intervals, prior_matrices, labels = sample\n",
    "    if intervals[-1] != 0:\n",
    "        num_unq_encs += 2\n",
    "        num_enc_gaps += 1\n",
    "    else:\n",
    "        num_unq_encs += 1\n",
    "        num_enc_gaps += 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_codes_to_strings(top_10_dict, all_int2str):\n",
    "    top_10_dict_translated = dict()\n",
    "    for key, values in top_10_dict.items():\n",
    "        code_1 = all_int2str[key[0]]\n",
    "        code_2 = all_int2str[key[1]]\n",
    "        top_10_dict_translated[(code_1, code_2)] = values\n",
    "    return top_10_dict_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_enc_edge_dict(enc_edge_dict, enc_count, all_int2str, top_n):\n",
    "    \"\"\"\n",
    "    Processes enc_edge_dict to compute 'pct', 'mean', and 'std' for each key.\n",
    "    Keeps only the top 10 key-value pairs with the highest 'pct'.\n",
    "\n",
    "    Parameters:\n",
    "    - enc_edge_dict (dict): Dictionary where keys are (node_i, node_j) tuples, \n",
    "                            and values are lists of float numbers.\n",
    "    - enc_count (int): Total number of encoding counts.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Processed dictionary with top 10 (node_i, node_j) pairs by 'pct'.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "\n",
    "    # Compute pct, mean, std for each key\n",
    "    for key, values in enc_edge_dict.items():\n",
    "        pct = len(values) / enc_count\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        \n",
    "        processed_dict[key] = {'pct': pct, 'mean': mean, 'std': std}\n",
    "\n",
    "    # Sort by 'pct' in descending order and keep only the top 10\n",
    "    top_n_dict = dict(sorted(processed_dict.items(), key=lambda x: x[1]['pct'], reverse=True)[:top_n])\n",
    "    \n",
    "    # translate the keys to strings\n",
    "    top_n_dict = translate_codes_to_strings(top_n_dict, all_int2str)\n",
    "\n",
    "    return top_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_intra_enc_edge_dict = process_enc_edge_dict(intra_enc_edge_dict, num_unq_encs, all_int2str_short, 8)\n",
    "top_inter_enc_edge_dict = process_enc_edge_dict(inter_enc_edge_dict, num_enc_gaps, all_int2str_short, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_label(label, max_length=50, wrap_at=25):\n",
    "    \"\"\"\n",
    "    Truncate label to max_length and wrap into multiple lines if needed.\n",
    "\n",
    "    Parameters:\n",
    "    - label (str): The text label.\n",
    "    - max_length (int): Maximum character length before truncating.\n",
    "    - wrap_at (int): Number of characters per line for wrapping.\n",
    "\n",
    "    Returns:\n",
    "    - str: Wrapped text with line breaks.\n",
    "    \"\"\"\n",
    "    label = label.split(\"|\")[-1]  # Keep only the last part after '|'\n",
    "    if len(label) > max_length:\n",
    "        label = label[:max_length] + \"...\"\n",
    "    return \"\\n\".join(textwrap.wrap(label, wrap_at))  # Wrap at spaces\n",
    "\n",
    "def plot_enc_edge_data(ax, data, title=\"Mean and Standard Deviation of Conditions\"):\n",
    "    \"\"\"\n",
    "    Plots mean values with standard deviation error bars for given condition pairs.\n",
    "\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The axis object where the plot will be drawn.\n",
    "    - data (dict): Dictionary with keys as (condition_1, condition_2) and values as:\n",
    "      {'pct': float, 'mean': float, 'std': float}\n",
    "    - title (str): Title of the subplot.\n",
    "    \"\"\"\n",
    "    # Sort data by 'pct' in descending order\n",
    "    sorted_data = sorted(data.items(), key=lambda x: x[1]['pct'], reverse=True)\n",
    "\n",
    "    # Extract labels and values\n",
    "    labels = [\n",
    "        f\"{wrap_label(k[1])} â†’\\n{wrap_label(k[0])} ({v['pct'] * 100:.2f}%)\"\n",
    "        for k, v in sorted_data\n",
    "    ]\n",
    "    means = [v['mean'] for k, v in sorted_data]  # X-axis mean values\n",
    "    std_devs = [v['std'] for k, v in sorted_data]  # Error bars (std)\n",
    "\n",
    "    # Y-axis positions\n",
    "    y_pos = np.arange(len(labels))\n",
    "\n",
    "    # Plot mean as blue dots\n",
    "    ax.scatter(means, y_pos, color='blue', label='Mean')\n",
    "\n",
    "    # Plot error bars (std deviation)\n",
    "    ax.errorbar(means, y_pos, xerr=std_devs, fmt='o', color='black', capsize=3)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels, fontsize=16)\n",
    "    ax.set_xlabel(\"Edge weight\")\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    ax.invert_yaxis()  # Highest pct at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten the labels\n",
    "code_name_short_map = {\n",
    "    \"mechanical ventilation\": \"Mechanical ventilation\",\n",
    "    \"organ system|cardiovascular\": \"Cardiovascular\",\n",
    "    \"respiratory failure|acute respiratory failure\": \"Acute respiratory failure\",\n",
    "    \"respiratory failure|hypoxemia\": \"Hypoxemia\",\n",
    "    \"arrest, respiratory (without cardiac arrest)\": \"Respiratory arrest\",\n",
    "    \"cardiac arrest (with or without respiratory arrest; for respiratory arrest see respiratory system)\": \"Cardiac arrest\",\n",
    "    \"norepinephrine > 0.1 micrograms/kg/min\": \"Norepinephrine\",\n",
    "    \n",
    "}\n",
    "\n",
    "top_intra_enc_edge_dict_short = {}\n",
    "top_inter_enc_edge_dict_short = {}\n",
    "for edge in top_intra_enc_edge_dict.keys():\n",
    "    node_1 = edge[0]\n",
    "    node_2 = edge[1]\n",
    "    if node_1 in code_name_short_map.keys():\n",
    "        node_1 = code_name_short_map[node_1]\n",
    "    if node_2 in code_name_short_map.keys():\n",
    "        node_2 = code_name_short_map[node_2]\n",
    "    top_intra_enc_edge_dict_short[(node_1, node_2)] = top_intra_enc_edge_dict[edge]\n",
    "for edge in top_inter_enc_edge_dict.keys():\n",
    "    node_1 = edge[0]\n",
    "    node_2 = edge[1]\n",
    "    if node_1 in code_name_short_map.keys():\n",
    "        node_1 = code_name_short_map[node_1]\n",
    "    if node_2 in code_name_short_map.keys():\n",
    "        node_2 = code_name_short_map[node_2]\n",
    "    top_inter_enc_edge_dict_short[(node_1, node_2)] = top_inter_enc_edge_dict[edge]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))  # Creating subplots\n",
    "plot_enc_edge_data(axes[0], top_intra_enc_edge_dict_short, title=\"eICU: Most Prevalent Intra-Encounter Edges\")\n",
    "plot_enc_edge_data(axes[1], top_inter_enc_edge_dict_short, title=\"eICU: Most Prevalent Inter-Encounter Edges\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('./eicu_edge_weights.tiff', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clincial Module Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization strategy here is that we choose some common dx and px code in ICU and see what codes are the most co-occuring with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_codes = [3, 4, 233, 5]\n",
    "analyse_codes_count = {code: 0 for code in analyse_codes}\n",
    "same_cluster_codes = {code: {} for code in analyse_codes}\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    pat_id, code_ints, pad_masks, causal_masks, intervals, prior_matrices, labels = sample\n",
    "    with torch.no_grad():\n",
    "        _, adj_matrices, cluster_assign, cluster_weights, _, _, _ = best_model(\n",
    "            code_ints.unsqueeze(0), \n",
    "            pad_masks.unsqueeze(0), \n",
    "            causal_masks.unsqueeze(0), \n",
    "            intervals.unsqueeze(0), \n",
    "            prior_matrices.unsqueeze(0)\n",
    "        )\n",
    "    code_ints = code_ints.detach().numpy()\n",
    "    cluster_hard = cluster_assign.argmax(dim=-1)[0].detach().numpy()\n",
    "\n",
    "    for code in analyse_codes:\n",
    "        if code in code_ints:\n",
    "            # Increment count for the code\n",
    "            analyse_codes_count[code] += 1\n",
    "\n",
    "            # Find all indices where code appears\n",
    "            code_indices = np.where(code_ints == code)[0]\n",
    "            \n",
    "            # Find the corresponding clusters\n",
    "            code_clusters = cluster_hard[code_indices]\n",
    "\n",
    "            # Find all codes in the same clusters\n",
    "            for cluster in np.unique(code_clusters):\n",
    "                cluster_codes = code_ints[np.where(cluster_hard == cluster)[0]]\n",
    "                \n",
    "                # Update same_cluster_codes dictionary\n",
    "                for cluster_code in set(cluster_codes):\n",
    "                    if cluster_code != code and cluster_code != pad_token:\n",
    "                        if cluster_code not in same_cluster_codes[code]:\n",
    "                            same_cluster_codes[code][cluster_code] = 0\n",
    "                        same_cluster_codes[code][cluster_code] += 1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert co-occurrence counts to percentages\n",
    "for code in same_cluster_codes:\n",
    "    if analyse_codes_count[code] > 0:\n",
    "        for co_code in same_cluster_codes[code]:\n",
    "            same_cluster_codes[code][co_code] /= analyse_codes_count[code]  # Normalize to percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Keep only the top 5 co-occurring codes for each analyse_code\n",
    "for code in same_cluster_codes:\n",
    "    sorted_codes = sorted(same_cluster_codes[code].items(), key=lambda x: x[1], reverse=True)  # Sort by percentage\n",
    "    same_cluster_codes[code] = dict(sorted_codes[:5])  # Keep only top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: translate the codes to strings\n",
    "translated_same_cluster_codes = {}\n",
    "for analyse_code, co_codes in same_cluster_codes.items():\n",
    "    translated_same_cluster_codes[all_int2str_short[analyse_code]] = {\n",
    "        all_int2str_short[co_code]: percentage\n",
    "        for co_code, percentage in co_codes.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_y_label(label):\n",
    "    parts = label.split(\"|\")\n",
    "    if parts[-1].strip().lower() == \"no\":  # If last part is \"no\", prepend \"No\" to the second last part\n",
    "        shortened =  \"No \" + parts[-2].strip().capitalize() if len(parts) > 1 else \"No\"\n",
    "    else:\n",
    "        shortened = parts[-1].strip().capitalize()  # Otherwise, capitalize the last part\n",
    "    \n",
    "    if shortened == \"No Thrombolytic therapy received within 24 hours\":\n",
    "        shortened = \"No thrombolytic therapy\"\n",
    "    return shortened \n",
    "\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "num_codes = len(translated_same_cluster_codes)\n",
    "num_cols = 4  # Fixed number of columns per row\n",
    "num_rows = math.ceil(num_codes / num_cols)  # Calculate required rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 6, num_rows * 6))  # Adjust figure size\n",
    "\n",
    "# Flatten axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each analyse_code\n",
    "for i, (code, co_codes_dict) in enumerate(translated_same_cluster_codes.items()):\n",
    "    co_codes = [format_y_label(c) for c in co_codes_dict.keys()]  # Format y labels\n",
    "    percentages = [value * 100 for value in co_codes_dict.values()]  # Convert to percentages\n",
    "\n",
    "    axes[i].barh(co_codes, percentages, color='skyblue')  # Use horizontal bars for better readability\n",
    "    axes[i].set_xlabel(\"%\")  # Label x-axis\n",
    "    axes[i].set_title(f\"{code.split('|')[-1].capitalize()}\", fontsize=19)  # Set title as analyse_code\n",
    "    axes[i].invert_yaxis()  # Invert y-axis for better readability\n",
    "    axes[i].tick_params(axis='y', labelsize=18)  # Enlarge y-axis labels\n",
    "    \n",
    "\n",
    "# Hide unused subplots (if any)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('./eicu_top5_cooccurrence.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
